{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFofdm5j_6lr"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tf\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from imutils import paths\n",
    "from torchmetrics import Dice\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epoch = 1\n",
    "data_path = \"DATASET/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "memory = True if device == 'cuda' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QAtpnsG_6lt"
   },
   "outputs": [],
   "source": [
    "def image_resize(img, size_x = 224, size_y = 224):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    problem = abs(height - width)\n",
    "    sz = [int(problem / 2), problem - int(problem / 2), 0]\n",
    "    if width >= height:\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, sz[0], sz[1], sz[2], sz[2], cv2.BORDER_CONSTANT)\n",
    "    else:\n",
    "        img = cv2.copyMakeBorder(\n",
    "            img, sz[2], sz[2], sz[0], sz[1], cv2.BORDER_CONSTANT)\n",
    "    img = cv2.resize(img, (size_x, size_y))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41w7w85H_6lu"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(dt.Dataset):\n",
    "    def __init__(self, imagPaths, maskPaths, transforms, transforms_affine=None):\n",
    "        self.imagPaths = imagPaths\n",
    "        self.maskPaths = maskPaths\n",
    "        self.transforms = transforms\n",
    "        self.transforms_affine = transforms_affine\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagPaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imag = cv2.imread(self.imagPaths[idx], cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.maskPaths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        imag = image_resize(imag)\n",
    "        mask = image_resize(mask)\n",
    "        if self.transforms_affine != None:\n",
    "            transformed = self.transforms_affine(image=imag, mask=mask)\n",
    "            imag = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        imag = self.transforms(imag)\n",
    "        mask = tf.Compose([tf.ToTensor()])(mask)\n",
    "        return imag, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTestDataset(dt.Dataset):\n",
    "    def __init__(self, imagPaths, maskPaths, transforms):\n",
    "        self.imagPaths = imagPaths\n",
    "        self.maskPaths = maskPaths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagPaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imag = cv2.imread(self.imagPaths[idx], cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.maskPaths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        imag = image_resize(imag)\n",
    "        mask = image_resize(mask)\n",
    "        real = tf.Compose([tf.ToTensor()])(imag)\n",
    "        mask = tf.Compose([tf.ToTensor()])(mask)\n",
    "        imag = self.transforms(imag)\n",
    "        return real, imag, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(imag_path, mask_path):\n",
    "    all_path_imag = sorted(list(paths.list_images(imag_path)))\n",
    "    all_path_mask = sorted(list(paths.list_images(mask_path)))\n",
    "    split = train_test_split(all_path_imag, all_path_mask, test_size=0.2)\n",
    "    (train_imag, test_imag) = split[:2]\n",
    "    (train_mask, test_mask) = split[2:]\n",
    "    return train_imag, test_imag, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(train_imag, train_mask, batch_size):\n",
    "    transform_none = tf.Compose([tf.ToTensor()])\n",
    "    imag_train = SegmentationDataset(train_imag, train_mask, transform_none)\n",
    "    train_loader = dt.DataLoader(imag_train, batch_size=batch_size, shuffle=True)\n",
    "    mean = torch.tensor([0., 0., 0.])\n",
    "    std = torch.tensor([0., 0., 0.])\n",
    "    for data in train_loader:\n",
    "        img, mask = data\n",
    "        mean += img.mean([0, 2, 3])\n",
    "        std += img.std([0, 2, 3])\n",
    "    mean /= len(train_loader)\n",
    "    std /= len(train_loader)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(train_imag, test_imag, train_mask, test_mask, batch_size, transform = tf.Compose([tf.ToTensor()]), transform_affine = A.Compose([A.HorizontalFlip(p=0)]), concat = None):\n",
    "    imag_train_trans = SegmentationDataset(train_imag, train_mask, transform, transform_affine)\n",
    "    imag_test_trans = SegmentationDataset(test_imag, test_mask, transform, transform_affine)\n",
    "    if concat:\n",
    "        imag_train = SegmentationDataset(train_imag, train_mask, transform)\n",
    "        imag_test = SegmentationDataset(test_imag, test_mask, transform)\n",
    "        imag_train_trans = dt.ConcatDataset([imag_train, imag_train_trans])\n",
    "        imag_test_trans = dt.ConcatDataset([imag_test, imag_test_trans])\n",
    "    dataset_train = dt.DataLoader(imag_train_trans, batch_size=batch_size, shuffle=True)\n",
    "    dataset_test = dt.DataLoader(imag_test_trans, batch_size=batch_size, shuffle=False)\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_batch_size = 4\n",
    "bad_path_imag = data_path + \"gen_smoke_bad\"\n",
    "bad_path_mask = data_path + \"gen_smoke_bad_masks\"\n",
    "bad_train_imag, bad_test_imag, bad_train_mask, bad_test_mask = get_data(bad_path_imag, bad_path_mask)\n",
    "bad_mean, bad_std = get_mean_std(bad_train_imag, bad_train_mask, bad_batch_size)\n",
    "bad_transform = tf.Compose([tf.ToTensor(), tf.Normalize(bad_mean, bad_std),\n",
    "                            tf.GaussianBlur(kernel_size=(7, 13), sigma=(0.25, 0.5))])\n",
    "bad_dataset_train, bad_dataset_test = get_dataset(bad_train_imag, bad_test_imag,\n",
    "                                                  bad_train_mask, bad_test_mask,\n",
    "                                                  bad_batch_size, bad_transform)\n",
    "\n",
    "blender_batch_size = 4\n",
    "blender_path_imag = data_path + \"blender_smoke\"\n",
    "blender_path_mask = data_path + \"blender_smoke_masks\"\n",
    "blender_train_imag, blender_test_imag, blender_train_mask, blender_test_mask = get_data(blender_path_imag, blender_path_mask)\n",
    "blender_mean, blender_std = get_mean_std(blender_train_imag, blender_train_mask, blender_batch_size)\n",
    "blender_transform = tf.Compose([tf.ToTensor(), tf.Normalize(blender_mean, blender_std)])\n",
    "blender_transform_affine = A.Compose([A.Perspective(scale=(0.05, 0.2), p=1.),\n",
    "                                      A.Affine(rotate=25, scale=(1, 1)),\n",
    "                                      A.HorizontalFlip(p=.5),\n",
    "                                      A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50, p=1)])\n",
    "blender_dataset_train, blender_dataset_test = get_dataset(blender_train_imag, blender_test_imag,\n",
    "                                                          blender_train_mask, blender_test_mask,\n",
    "                                                          blender_batch_size, blender_transform,\n",
    "                                                          blender_transform_affine, True)\n",
    "\n",
    "good_batch_size = 4\n",
    "good_path_imag = data_path + \"orig_smoke\"\n",
    "good_path_mask = data_path + \"orig_smoke_masks\"\n",
    "good_train_imag, good_test_imag, good_train_mask, good_test_mask = get_data(good_path_imag, good_path_mask)\n",
    "good_mean, good_std = get_mean_std(good_train_imag, good_train_mask, good_batch_size)\n",
    "good_transform = tf.Compose([tf.ToTensor(), tf.Normalize(good_mean, good_std)])\n",
    "good_transform_affine = A.Compose([A.Perspective(scale=(0.05, 0.3), p=1.),\n",
    "                                   A.Affine(rotate=25, scale=(0.9, 0.9)),\n",
    "                                   A.HorizontalFlip(p=.5)])\n",
    "good_dataset_train, good_dataset_test = get_dataset(good_train_imag, good_test_imag,\n",
    "                                                    good_train_mask, good_test_mask,\n",
    "                                                    good_batch_size, good_transform,\n",
    "                                                    good_transform_affine, True)\n",
    "\n",
    "test_batch_size = 1\n",
    "test_Paths = sorted(list(paths.list_images(data_path + 'test')))\n",
    "test_masks_Path = sorted(list(paths.list_images(data_path + 'test_mask')))\n",
    "test_transform = tf.Compose([tf.ToTensor(), tf.Normalize(good_mean, good_std)])\n",
    "test_imag = SegmentationTestDataset(test_Paths, test_masks_Path, test_transform)\n",
    "test_dataset = dt.DataLoader(test_imag, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot(H):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(np.arange(1, len(H[\"loss\"])+1), H[\"loss\"], label=\"loss\")\n",
    "    plt.plot(np.arange(1, len(H[\"accu\"])+1), H[\"accu\"], label=\"accu\")\n",
    "    plt.title(\"Training Loss and Accu\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_wA9sgn_6lv"
   },
   "outputs": [],
   "source": [
    "model = smp.MAnet(\n",
    "    encoder_name=\"efficientnet-b7\",\n",
    "    encoder_depth=5,\n",
    "    encoder_weights='imagenet',\n",
    "    activation='sigmoid'\n",
    ").to(device)\n",
    "\n",
    "loss_func = torch.nn.BCELoss().to(device)\n",
    "accu_func = Dice().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOecnvYR_6lv"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, learning_rate=1e-4, num_epoch=5, scheduler=5):\n",
    "    global epoch, model, H\n",
    "    best_epoch = 0\n",
    "    best_epoch_ind = 0\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    for e in range(num_epoch):\n",
    "        model.train()\n",
    "        totalLoss = 0\n",
    "        totalAccu = 0\n",
    "        if (e + 1) % scheduler == 0:\n",
    "            learning_rate /= scheduler\n",
    "        for (i, (x, y)) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred.squeeze(1), y.squeeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            totalLoss += loss\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for (x, y) in test_loader:\n",
    "                (x, y) = (x.to(device), y.to(device))\n",
    "                pred = model(x)\n",
    "                accu = accu_func(pred.squeeze(1), y.squeeze(1) > 0.5)\n",
    "                totalAccu += accu\n",
    "\n",
    "        avgLoss = totalLoss / len(train_loader)\n",
    "        avgAccu = totalAccu / len(test_loader)\n",
    "        H[\"loss\"].append(avgLoss.cpu().detach().numpy())\n",
    "        H[\"accu\"].append(avgAccu.cpu().detach().numpy())\n",
    "        print(\"[INFO] EPOCH: {}\".format(epoch))\n",
    "        print(\"loss: {:.4f}, accu: {:.4f}\\n\".format(avgLoss, avgAccu))\n",
    "\n",
    "        if best_epoch < avgAccu:\n",
    "            best_epoch = avgAccu\n",
    "            best_epoch_ind = epoch\n",
    "        torch.save(model.state_dict(),\n",
    "                   \"EPOCH/model_weights-\" + str(epoch) + \".pth\")\n",
    "        epoch += 1\n",
    "    model.load_state_dict(torch.load(\"EPOCH/model_weights-\" +\n",
    "                          str(best_epoch_ind) + \".pth\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "es_m5GSn_6lw",
    "outputId": "a38b96c0-a4ce-4250-8e12-34c24b5ff560"
   },
   "outputs": [],
   "source": [
    "H = {\"loss\": [], \"accu\": []}\n",
    "train(bad_dataset_train, bad_dataset_test, num_epoch=5)\n",
    "draw_plot(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {\"loss\": [], \"accu\": []}\n",
    "train(blender_dataset_train, blender_dataset_test, num_epoch=10)\n",
    "draw_plot(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = {\"loss\": [], \"accu\": []}\n",
    "train(good_dataset_train, good_dataset_test, num_epoch=10)\n",
    "draw_plot(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalAccu = 0\n",
    "fig, ax = plt.subplots(len(test_Paths), 3, figsize=(20, 200))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for (i, (r, x, y)) in enumerate(test_dataset):\n",
    "        (r, x, y) = (r.to(device), x.to(device), y.to(device))\n",
    "        p = model(x)\n",
    "        ax[i][0].imshow(cv2.cvtColor(np.clip(\n",
    "            r.squeeze(0).cpu().numpy().transpose(1, 2, 0), 0, 1), cv2.COLOR_RGB2BGR))\n",
    "        ax[i][1].imshow(cv2.cvtColor(p.squeeze(0).cpu().numpy(\n",
    "        ).round().transpose(1, 2, 0), cv2.IMREAD_GRAYSCALE))\n",
    "        ax[i][2].imshow(cv2.cvtColor(\n",
    "            y.squeeze(0).cpu().numpy().transpose(1, 2, 0), cv2.IMREAD_GRAYSCALE))\n",
    "        totalAccu += accu_func(p.squeeze(1), y.squeeze(1) > 0.5)\n",
    "avgAccu = totalAccu / len(test_dataset)\n",
    "print(\"AVG accu: {:.4f}\".format(avgAccu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"MegaKal.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"badDataset.pth\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(file_path, frames, fps):\n",
    "    num, w, h, ch = frames.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "    writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))\n",
    "\n",
    "    for frame in frames:\n",
    "        writer.write(np.uint8(frame))\n",
    "\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(prev_p, p, r=10):\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if(prev_p[max(0,i-r):min(p.shape[0], i + r),max(0,j-r):min(p.shape[1], j + r)].sum() == 0):\n",
    "                p[i,j] = 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(file_path, skip=10):\n",
    "    global model\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    transforms = tf.Compose([tf.ToTensor(), tf.Normalize(good_mean, good_std)])\n",
    "    images = []\n",
    "    num_frame = 0\n",
    "    prev_p = np.nan\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        while (video.isOpened()):\n",
    "            num_frame += 1\n",
    "            flag, x = video.read()\n",
    "            if flag:\n",
    "                img_cpy = np.copy(x)\n",
    "                img_cpy = image_resize(img_cpy, max(img_cpy.shape[1],img_cpy.shape[0]),\n",
    "                                                max(img_cpy.shape[1],img_cpy.shape[0]))\n",
    "                img_cpy = cv2.normalize(img_cpy, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_16U)\n",
    "                \n",
    "                x = image_resize(x)\n",
    "                p = model(transforms(x).to(device).unsqueeze(0))\n",
    "                p = p.cpu().numpy().squeeze()\n",
    "                x = cv2.normalize(x, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_16U)\n",
    "                p = np.expand_dims(cv2.normalize(\n",
    "                    p.round(), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_16U), axis=2)\n",
    "\n",
    "                if num_frame > skip:\n",
    "                    p = check_dist(prev_p, p)\n",
    "                    prev_p = p\n",
    "                elif num_frame < skip:\n",
    "                    if (num_frame == 1):\n",
    "                        prev_p = p\n",
    "                    else:\n",
    "                        prev_p += p\n",
    "                else:\n",
    "                    prev_p += prev_p\n",
    "                    prev_p //= skip\n",
    "\n",
    "                p = image_resize(p, max(img_cpy.shape[1],img_cpy.shape[0]),\n",
    "                                    max(img_cpy.shape[1],img_cpy.shape[0]))\n",
    "                p = np.expand_dims(cv2.blur(p, (13, 7)), axis=2)\n",
    "                \n",
    "                img_cpy[:, :, 1] = np.where(\n",
    "                    p[:, :, 0] < 128, img_cpy[:, :, 1], img_cpy[:, :, 1] * 0.7)\n",
    "                img_cpy[:, :, 0] = np.where(\n",
    "                    p[:, :, 0] < 128, img_cpy[:, :, 0], img_cpy[:, :, 0] * 0.7)\n",
    "                img_cpy[:, :, 2] = np.where(\n",
    "                    p[:, :, 0] < 128, img_cpy[:, :, 2], img_cpy[:, :, 2] * 0.7 + p[:, :, 0] * 0.3)\n",
    "                images.append(img_cpy)\n",
    "\n",
    "                if (num_frame == 300):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    video.release()\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = read_video('DATASET/video/6.mp4')\n",
    "write_video('vidMOD.avi', images, 30)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
